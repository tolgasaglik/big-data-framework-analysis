SLURM_JOBID  = 230473
SLURM_JOB_NODELIST = iris-[104-106]
SLURM_NNODES = 3
SLURM_NTASK  = 2
Submission directory = /mnt/irisgpfs/users/tsaglik/git/github.com/ULHPC/tutorials/bigdata/runs
starting org.apache.spark.deploy.master.Master, logging to /home/users/tsaglik/.spark/logs/spark-230473-org.apache.spark.deploy.master.Master-1-iris-104.out
==========================================
============== Spark Master ==============
==========================================
url: spark://iris-104:7077
Web UI: http://iris-104:8082

===========================================
============ 2 Spark Workers ==============
===========================================
export SPARK_HOME=$EBROOTSPARK
export MASTER_URL=spark://iris-104:7077
export SPARK_DAEMON_MEMORY=4096m
export SPARK_WORKER_CORES=28
export SPARK_WORKER_MEMORY=110592m
export SPARK_EXECUTOR_MEMORY=110592m

 - create slave launcher script '/home/users/tsaglik/.spark/worker/spark-start-slaves-230473.sh'
===========================================
=> running '/home/users/tsaglik/.local/easybuild/software/devel/Spark/2.4.0-Hadoop-2.7-Java-1.8.0_162/examples/src/main/python/pi.py 1000' on Spark cluster
=> output file: result_Spark-230473.out
spark-submit     --master spark://iris-104:7077     --conf spark.driver.memory=4096m     --conf spark.executor.memory=110592m     --conf spark.python.worker.memory=110592m     /home/users/tsaglik/.local/easybuild/software/devel/Spark/2.4.0-Hadoop-2.7-Java-1.8.0_162/examples/src/main/python/pi.py 1000

srun: Warning: can't run 2 processes on 3 nodes, setting nnodes to 2
===========================================
=> check the output file: result_Spark-230473.out

=> stopping worker(s)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
=> stopping spark master
stopping org.apache.spark.deploy.master.Master
